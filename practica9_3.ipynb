{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXROQV9uZ2n"
      },
      "source": [
        "# Práctica 9 Parte 3: Desarrollando un modelo de lenguaje para generar texto\n",
        "\n",
        "Un modelo de lenguaje puede predecir la siguiente palabra de una secuencia basándose en palabras observadas anteriormente. Las redes neuronales son el método más utilizado para desarrollar este tipo de modelos porque pueden usar una representación donde palabras con significados similares tienen representaciones similares. \n",
        "\n",
        "En esta parte de la práctica vamos a ver cómo generar uno de esos modelos. \n",
        "\n",
        "Este notebook está basado en el libro Deep Learning for Natural Language Processing de Jason Brownlee. \n",
        "\n",
        "Es importante que tengas activado el uso de **GPU** en el notebook de colab (menú Edit -> Notebook Settings -> Hardware accelerator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7fHn2VKwO0E"
      },
      "source": [
        "## La República de Platón\n",
        "\n",
        "Nuestro modelo de lenguaje va a estar basado en la república de Platón. Este libro está estructurado en forma de una conversación que trata el tema del orden y la justicia dentro de una ciudad. El texto completo está disponible para el dominio público dentro del [proyecto Gutenberg](http://www.gutenberg.org/).\n",
        "\n",
        "Este libro de Platón está disponible en varios formatos en el [proyecto Gutenberg](http://www.gutenberg.org/cache/epub/1497/pg1497.txt). La versión que nos interesa a nosotros es la versión ASCII del libro. Con la siguiente instrucción puedes descargar el libro donde se han eliminado la portada y la contraportada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpmhqeiuuFYV",
        "outputId": "53a25389-e79f-46a4-8c84-20fbd0c46e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 00:56:19--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657826 (642K) [text/plain]\n",
            "Saving to: ‘republic.txt’\n",
            "\n",
            "republic.txt        100%[===================>] 642.41K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-06-12 00:56:19 (14.1 MB/s) - ‘republic.txt’ saved [657826/657826]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt -O republic.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_hckdhxWdz"
      },
      "source": [
        "## Preparación de los datos\n",
        "\n",
        "Vamos a preparar los datos para construir nuestro modelo. \n",
        "\n",
        "### Revisando el texto\n",
        "\n",
        "Vamos a comenzar revisando parte del texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn3g3XJYxSTO",
        "outputId": "d9692ac8-b4dc-4cee-b33c-64f023de235c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK I.\r\n",
            "\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\n",
            "Artemis.); and also because I wanted to see in what manner they would\r\n",
            "celebrate the festival, which was a new thing. I was delighted with the\r\n",
            "procession of the inhabitants; but that of the Thracians was equally,\r\n",
            "if not more, beautiful. When we had finished our prayers and viewed the\r\n",
            "spectacle, we turned in the direction of the city; and at that instant\r\n",
            "Polemarchus the son of Cephalus chanced to catch sight of us from a\r\n",
            "distance as we were starting on our way home, and told his servant to\r\n",
            "run and bid us wait for him. The servant took hold of me by the cloak\r\n",
            "behind, and said: Polemarchus desires you to wait.\r\n",
            "\r\n",
            "I turned round, and asked him where his master was.\r\n",
            "\r\n",
            "There he is, said the youth, coming after you, if you will only wait.\r\n",
            "\r\n",
            "Certainly we will, said Glaucon; and in a few minutes Polemarchus\r\n",
            "appeared, and with him Adeimantus, Glaucon's brother, Niceratus the son\r\n",
            "of Nicias, and several others who had been at the procession.\r\n",
            "\r\n",
            "Polemarchus said to me: I perceive, Socrates, that you and your\r\n",
            "companion are already on your way to the city.\r\n",
            "\r\n",
            "You are not far wrong, I said.\r\n",
            "\r\n",
            "But do you see, he rejoined, how many we are?\r\n",
            "\r\n",
            "Of course.\r\n"
          ]
        }
      ],
      "source": [
        "!head -30 republic.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4uFv8RWzVn-"
      },
      "source": [
        "A partir de un rápido vistazo al fragmento de texto anterior podemos ver ciertas cuestiones que tendremos que procesar:\n",
        "- Las cabeceras de los capítulos.\n",
        "- Muchos signos de puntuación.\n",
        "- Nombres extraños.\n",
        "- Algunos monólogos muy largos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAfXxOz0GQT"
      },
      "source": [
        "### Cargando el texto\n",
        "\n",
        "El primer paso consiste en cargar el texto en memoria. Podemos desarrollar una pequeña función que se encargue de esto. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dDTRFi4xmBr"
      },
      "outputs": [],
      "source": [
        "def load_doc(filename):\n",
        "  # Abrimos el fichero en modo lectura\n",
        "  file = open(filename,'r')\n",
        "  # Leemos el texto completo\n",
        "  text = file.read()\n",
        "  # Cerramos el fichero\n",
        "  file.close()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SdKtPFM0aZ-"
      },
      "source": [
        "Usando dicha función podemos cargar nuestro fichero del siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_oUjl_Q0Zgn"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic.txt'\n",
        "doc = load_doc(in_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVfi8WHI0kKb"
      },
      "source": [
        "Ahora podemos mostrar parte de dicho texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZRb4q6W0jQT",
        "outputId": "38524ca1-46da-439b-f0a5-e07c923b7ec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK I.\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
            "Artemis.); and also because I wanted to see in what\n"
          ]
        }
      ],
      "source": [
        "print(doc[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4PZ32G0reo"
      },
      "source": [
        "### Limpiando el texto\n",
        "\n",
        "Ahora necesitamos transformar el texto en bruto a una secuencia de tokens (o palabras) que podamos usar para entrenar nuestro modelo. \n",
        "\n",
        "Vamos a aplicar las siguientes operaciones para limpiar nuestro texto:\n",
        "- Reemplazar todas las ocurrencias de '-' con un espacio en blanco de manera que podamos partir mejor las palabras.\n",
        "- Partir las palabras basándonos en espacios en blanco.\n",
        "- Eliminar todos los símbolos de puntuación.\n",
        "- Eliminar todas las palabras que no son alfabéticas. \n",
        "- Normalizar todas las palabras a minúsculas.\n",
        "\n",
        "La mayoría de estas transformaciones tienen como objetivo reducir el tamaño del vocabulario. Un tamaño de vocabulario excesivamente grande es un problema cuando se intenta crear modelos de lenguaje. Vocabularios pequeños producen modelos más pequeños que se entrenan más rápidos.\n",
        "\n",
        "Vamos a implementar todas las operaciones de limpieza en la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsZiyRsE0niT"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def clean_doc(doc):\n",
        "  # Reemplazar '--' con un espacio en blanco ' '\n",
        "  doc = doc.replace('--',' ')\n",
        "  # Partir palabras basándonos en espacios en blanco\n",
        "  tokens = doc.split()\n",
        "  # Vamos a escapar las palabras para poder filtrarlas por caracteres\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # Eliminamos los símbolos de puntuación\n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "  # Eliminamos elementos que nos son alfabéticos\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # Convertimos a minúsculas\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVV2RsWB2kOD"
      },
      "source": [
        "Procedemos a limpiar nuestro documento y a continuación mostramos algunas estadísticas sobre nuestro vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUUCtGKP2jdA",
        "outputId": "936a1c3a-adab-4f0d-b93c-d7647aba5b28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n"
          ]
        }
      ],
      "source": [
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOBzRk232uAv",
        "outputId": "837f6d82-0c96-4d43-e82a-8e00af4c484b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ]
        }
      ],
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvkoWbVh25cR"
      },
      "source": [
        "Es decir, nuestro modelo consta de una 7500 palabras. Este tamaño de vocabulario es pequeño y va a ser manejable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odlhVwpz3CQi"
      },
      "source": [
        "### Guardando el texto limpio\n",
        "\n",
        "Vamos a organizar la larga lista de tokens en secuencias de 50 palabras de entrada y 1 palabra de salida (esto servirá para luego entrenar nuestro modelo). \n",
        "\n",
        "Este proceso lo implementamos con la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtEe2J7E21am"
      },
      "outputs": [],
      "source": [
        "def organize_tokens(tokens,input_len=50,output_len=1):\n",
        "  length = input_len + output_len\n",
        "  sequences = list()\n",
        "  for i in range(length,len(tokens)):\n",
        "    # Elegimos la secuencia de tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # Convertimos la secuencia en una línea\n",
        "    line = ' '.join(seq)\n",
        "    # Almacenamos el resultado\n",
        "    sequences.append(line)\n",
        "  return sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6upZchl37wM"
      },
      "source": [
        "Organizamos nuestros tokens. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpcwuFBp35XJ"
      },
      "outputs": [],
      "source": [
        "lines = organize_tokens(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXyDfPjA4Bfr"
      },
      "source": [
        "Ahora vamos a guardar las secuencias en un nuevo fichero para poder cargarlo en el futuro. Para ello nos definimos la siguiente función que guardará cada elemento de la secuencia en una línea del fichero. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWxPoCvn4AfS"
      },
      "outputs": [],
      "source": [
        "def save_doc(lines,filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtdjRviE4Wfd"
      },
      "source": [
        "Podemos llamar a la función anterior para guardar nuestro fichero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbncNaRo4T74"
      },
      "outputs": [],
      "source": [
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(lines,out_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giGMZ3yW4m9O"
      },
      "source": [
        "Podemos ver parte de dicho fichero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hitWeMb84e1R",
        "outputId": "233202bb-7b98-45a8-f9f1-40f9a2724ce3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n",
            "i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted\n",
            "i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with\n",
            "went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the\n",
            "down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession\n"
          ]
        }
      ],
      "source": [
        "!head -5 republic_sequences.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Do4ht04u3Q"
      },
      "source": [
        "## Entrenando el modelo de lenguaje\n",
        "\n",
        "Vamosa  entrenar ahora nuestro modelo a partir de los datos que hemos preparado. Dicho modelo tendrá ciertas características:\n",
        "- Usará una representación para las palabras de manera que palabras diferentes con significados similares tendrán una representación similar.\n",
        "- La representación será aprendida al mismo tiempo que se aprende el modelo.\n",
        "- Aprenderá a predecir la probabilidad de la siguiente palabra a partir del contexto de las últimas 100 palabras.\n",
        "\n",
        "En concreto para implementar este modelo vamos a usar una capa de Embedding para aprender la representación de las palabras, y una red neuronal recurrente con capas LSTM para predecir nuevas palabras basándonos en el contexto. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss_vMHL65cx2"
      },
      "source": [
        "### Cargando las secuencias\n",
        "\n",
        "Podemos comenzar cargando las secuencias que hemos guardado anteriormente. En este caso este paso no sería necesario ya que el proceso de generación de las secuencias es bastante rápido, pero si estamos trabajando con un dataset más grande sí que puede ser conveniente. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mChmy3q04rJq"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Au4s6vM7UED"
      },
      "source": [
        "### Codificando las secuencias\n",
        "\n",
        "Las capas de Embedding esperan que las secuencias de entrada estén compuestas de vectores de enteros. Para ello vamos a identificar cada palabra de nuestro vocabulario con un entero único y codificarlo en una secuencia de entrada. En el futuro cuando vayamos a realizar las predicciones tendremos que realizar el proceso inverso.\n",
        "\n",
        "Para llevar a cabo este proceso de tokenización vamos a usar la API de Keras del siguiente modo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVH8p7Cv52Zo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_odT9JZ710u"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEQoF2S478z9"
      },
      "source": [
        "Ahora podemos acceder a los identificadores de cada palabra usando el atributo ``word_index`` del objeto ``Tokenizer`` que hemos creado. \n",
        "\n",
        "Además debemos determinar el tamaño de nuestro vocabulario para definir la capa de embedding. En concreto, a las palabras de nuestro vocabulario se les han asignado valores entre 1 y el número total de palabras de nuestro vocabulario. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byu4Vfe075Aj"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaF1yDTF9elO"
      },
      "source": [
        "### Secuencias de entrada y salida\n",
        "\n",
        "Una vez que tenemos codificadas nuestras secuencias tenemos que separarlas en elementos de entrada ($X$) y de salida ($y$). Después de realizar la separación debemos codificar cada palabra usando el método one-hot. Este proceso lo llevaremos a cabo mediante la función ``to_categorical()`` de Keras.\n",
        "Finalmente necesitamos especificar cómo de largas serán las secuencias de entrada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SekaGTwl9eCl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array \n",
        "\n",
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUMEkkh-gSm"
      },
      "source": [
        "### Entrenando el modelo\n",
        "\n",
        "Ahora podemos definir nuestro modelo que constará de una capa de Embedding, seguida de dos capas LSTM y terminando con una red completamente conectada. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs_7VIsS-ODn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def define_model(vocab_size,seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "  model.add(LSTM(100,return_sequences=True))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(vocab_size,activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMo-mV9E_cco"
      },
      "source": [
        "Pasamos a entrenar nuestro modelo. Como este proceso es bastante costoso (incluso usando GPUs) en la siguiente sección se proporcionan los ficheros necesarios para usar el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFxHVuAuCdzC",
        "outputId": "40d9f30b-2f38-4794-b1c7-ab39a86f99a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "927/927 [==============================] - 21s 15ms/step - loss: 6.1479 - accuracy: 0.0726\n",
            "Epoch 2/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 5.6739 - accuracy: 0.1072\n",
            "Epoch 3/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 5.4375 - accuracy: 0.1288\n",
            "Epoch 4/100\n",
            "230/927 [======>.......................] - ETA: 10s - loss: 5.3029 - accuracy: 0.1357"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d497f858600f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.fit(X,y,batch_size=128,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EeUlPGxEifh"
      },
      "source": [
        "Una vez entrenado podemos guardar los pesos del modelo y el tokenizador. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKc5WObR_maO"
      },
      "outputs": [],
      "source": [
        "model.save_weights('./model.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdzX5deoEuiw"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "dump(tokenizer, open('tokenizer.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rnE1BOhEuIz"
      },
      "source": [
        "## Usando el modelo\n",
        "\n",
        "Como has podido ver en el paso anterior, el proceso de entrenar este tipo de modelos es muy costoso, por lo que puedes descargar los ficheros necesarios para usar el modelo desde el siguiente enlace. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJozw5qBFUoO",
        "outputId": "31e60f37-2266-4bde-b5d8-43af9db29688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-11 23:06:56--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101248 (4.9M) [application/octet-stream]\n",
            "Saving to: ‘model.h5’\n",
            "\n",
            "model.h5            100%[===================>]   4.86M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-06-11 23:06:57 (63.1 MB/s) - ‘model.h5’ saved [5101248/5101248]\n",
            "\n",
            "--2022-06-11 23:06:57--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353379 (345K) [application/octet-stream]\n",
            "Saving to: ‘tokenizer.pkl’\n",
            "\n",
            "tokenizer.pkl       100%[===================>] 345.10K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-06-11 23:06:57 (5.92 MB/s) - ‘tokenizer.pkl’ saved [353379/353379]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5 -O model.h5\n",
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl -O tokenizer.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbxpdyp4FZ0t"
      },
      "source": [
        "### Cargando los datos\n",
        "\n",
        "Comenzamos cargando nuestros datos al igual que antes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwREuZMiFfVT"
      },
      "outputs": [],
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQIu2LzlFl5v"
      },
      "source": [
        "Necesitamos este texto para elegir una secuencia de inicio que será la entrada para nuestro modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-IslrBxFlnA"
      },
      "outputs": [],
      "source": [
        "seq_length = len(lines[0].split())-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejCQcIN_Fxr2"
      },
      "source": [
        "### Cargando el modelo\n",
        "\n",
        "Vamos a cargar el modelo y a fijar los pesos. Notar que para este paso ya no necesitamos el uso de TPU, y que el modelo podría ser usado en cualquier ordenador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usmQZBvYF7Q1"
      },
      "outputs": [],
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.load_weights('./model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uvlOps3GG05"
      },
      "source": [
        "También necesitamos cargar el tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xib_xurDGIpL"
      },
      "outputs": [],
      "source": [
        "from pickle import load\n",
        "tokenizer = load(open('tokenizer.pkl','rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XrPda8NGUv7"
      },
      "source": [
        "### Generando texto\n",
        "\n",
        "El primer paso para generar el texto consiste en preparar una entrada, para lo cual elegiremos una línea aleatoria del texto. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EDkPYM5Gdmz",
        "outputId": "af1f5ec1-7778-41bd-84c6-cda88df415ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done but is war an art so easily acquired that a man may be a warrior who is also a husbandman or shoemaker or other artisan although no one in the world would be a good dice or draught player who merely took up the game as a recreation and had\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from random import randint\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZO-Teo5Go9O"
      },
      "source": [
        "A continuación podemos generar nuevas palabras una por una. Primero, el texto debe codificarse usando el tokenizer que hemos cargado anteriormente. Ahora el modelo puede predecir nuevas palabras usando el método ``predict_classes()`` que devuelve el índice de la palabra con probabilidad más alta. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mNpLQAtHMEY"
      },
      "source": [
        "Esta palabra se añade a nuestro texto inicial y se repite el proceso. Notar que esta secuencia va a ir creciendo por lo que tendremos que truncarla, para lo que utilizamos la función ``pad_sequences()`` de Keras. Todo este proceso se puede implementar con la siguiente función. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4J29cLaHZx-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n",
        "    yhat = model.predict(encoded,verbose=0)\n",
        "    yhat=np.argmax(yhat,axis=1)\n",
        "    out_word = ''\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h6dQKHIIIot"
      },
      "source": [
        "Ahora podemos generar una nueva secuencia usando el siguiente código. Cada vez que lo ejecutemos obtendremos un resultado distinto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FUn97EmILPG",
        "outputId": "1d4f3fd2-5e4b-4808-b43c-e5275a44d97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in introducing them to dialectic certainly there is a danger lest they should taste the dear delight too early for youngsters as you may have observed when they first get the taste in their mouths argue for amusement and are always contradicting and refuting others in imitation of those who refute\n",
            "\n",
            "them and he will be preserved in his head and says that the other rascalities and the other creative and constructive city are the just man and the other allurements of justice and therefore i said the question is the better and exhibits to them his life in finding money\n"
          ]
        }
      ],
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model,tokenizer,seq_length,seed_text,50)\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlmSJGumIepK"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Elige tu propio libro del proyecto Gutenberg (es posible usar libros en [español](https://www.gutenberg.org/browse/languages/es)) y crea tu propio modelo de lenguaje. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3jsZvvYI21o"
      },
      "outputs": [],
      "source": [
        "don_comedia = 'Divina comedia.txt'\n",
        "doc2 = load_doc(don_comedia)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens2 = clean_doc(doc2)\n",
        "lines2 = organize_tokens(tokens2)"
      ],
      "metadata": {
        "id": "Sdv6KrDJB-Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salida_comedia = 'Comedia_graciosa.txt'\n",
        "save_doc(lines2,salida_comedia)"
      ],
      "metadata": {
        "id": "x5i6s8olKovJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entrada_comedia = 'Comedia_graciosa.txt'\n",
        "doc2 = load_doc(entrada_comedia)\n",
        "lines2 = doc.split('\\n')"
      ],
      "metadata": {
        "id": "Z0EOQnHwL263"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = Tokenizer()\n",
        "tokenizer2.fit_on_texts(lines2)\n",
        "sequences2 = tokenizer2.texts_to_sequences(lines2)\n",
        "comedia_size = len(tokenizer2.word_index) + 1 "
      ],
      "metadata": {
        "id": "fPVmOe4BMTS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=comedia_size)\n",
        "comedia_length = X.shape[1]"
      ],
      "metadata": {
        "id": "AXPowm5LNgeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(comedia_size,comedia_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "  model.add(LSTM(100,return_sequences=True))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(vocab_size,activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "DxXeAiBsMpaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelComedia = define_model(comedia_size,comedia_length)\n",
        "modelComedia.fit(X,y,batch_size=128,epochs=100)"
      ],
      "metadata": {
        "id": "ABtNjIqIaKZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelComedia.save_weights('./modelComedia.h5', overwrite=True)"
      ],
      "metadata": {
        "id": "oyDis8Qea8zx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = load(open('tokenizer.pkl','rb'))\n",
        "seed_text = lines2[randint(0,len(lines2))]\n",
        "print(seed_text + '\\n')"
      ],
      "metadata": {
        "id": "6RkPGgbdbBn5",
        "outputId": "15f65dd0-783b-4337-d657-1a6fb76ff3d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kindly answer for the edification of the company and of myself glaucon and the rest of the company joined in my request and thrasymachus as any one might see was in reality eager to speak for he thought that he had an excellent answer and would distinguish himself but at first\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = lines2[randint(0,len(lines2))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(modelComedia,tokenizer,comedia_length,seed_text,50)\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "FIKIcY_6bHu6",
        "outputId": "75d3c397-7bf5-45b9-c1aa-efc8a3b39c91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and love are fled away there was a good time once but now that is gone and life is no longer life some complain of the slights which are put upon them by relations and they will tell you sadly of how many evils their old age is the cause but\n",
            "\n",
            "in the tyrannical state socrates and the spirits arrived in the city of which the senses are imposed upon by distance and by painting or fellowsailors within the end of the visible and assuredly with husbandry with the other particular things of which we may write and speak of false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG188-6TI3ph"
      },
      "source": [
        "Recuerda guardar este notebook en tu repositorio usando la opción \"Save in GitHub\" del menú File."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "practica9_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}